{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJO-2CLihs3"
      },
      "source": [
        "# Significance Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-80Jet0WRPFD"
      },
      "source": [
        "Evaluate the performance of the generated answers in the RAG-based QA system.\n",
        "\n",
        "For evaluation metrics, we use 3 metrics: answer recall, exact match, and F1 score frollowing the setting in\n",
        "the SQuAD paper (https://arxiv.org/pdf/1606.05250)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ActBx_a9OpKQ",
        "outputId": "20318a37-cdb1-4611-e5bf-869b3fe4604f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Kansas'], ['3 years'], ['1897'], ['Several'], ['Appalachia']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 9.446419502285424, 'F1 Score': 16.01480979249139, 'Answer Recall': 16.77409508429453}\n",
            "Results saved to ./results/tests/llama3_baseline.json\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "from collections import Counter\n",
        "import logging\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# Configure logging to display information about the script's execution\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def clean_answer(s):\n",
        "    \"\"\"\n",
        "    normalize an answer.\n",
        "    - Converts the text to lowercase.\n",
        "    - Removes punctuation, articles ('a', 'an', 'the'), and extra whitespace.\n",
        "    - Returns the cleaned text.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        # Remove articles using a regex pattern\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        # Fix extra whitespace by joining split words\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        # Remove punctuation characters\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        # Convert text to lowercase\n",
        "        return str(text).lower()\n",
        "\n",
        "    # Apply all cleaning steps\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"\n",
        "    Tokenize a string by cleaning it and splitting it into words.\n",
        "    - Returns a list of tokens.\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return clean_answer(s).split()\n",
        "\n",
        "def compute_exact_match_single(gold_answer_list, generated_answer):\n",
        "    \"\"\"\n",
        "    Check if the generated answer exactly matches any of the gold answers.\n",
        "    - Cleans both the generated answer and the gold answers.\n",
        "    - Returns True if there is an exact match, otherwise False.\n",
        "    \"\"\"\n",
        "    cleaned_generated = clean_answer(generated_answer)\n",
        "    return any(clean_answer(gold) == cleaned_generated for gold in gold_answer_list)\n",
        "\n",
        "def compute_exact_match(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Compute the exact match score.\n",
        "    - Compares each generated answer with its corresponding gold answers.\n",
        "    - Returns the percentage of exact matches.\n",
        "    \"\"\"\n",
        "    exact_match = sum(compute_exact_match_single(gold, gen) for gold, gen in zip(gold_answers, generated_answers))\n",
        "    return 100 * exact_match / len(gold_answers)\n",
        "\n",
        "def compute_recall_f1_single(args):\n",
        "    \"\"\"\n",
        "    Compute recall and F1 score for a single pair of gold and generated answers.\n",
        "    - Tokenizes the gold and generated answers.\n",
        "    - Calculates precision, recall, and F1 score based on token overlap.\n",
        "    - Returns the maximum recall and F1 score across all gold answers.\n",
        "    \"\"\"\n",
        "    gold_answer_list, generated_answer = args\n",
        "\n",
        "    # Tokenize the generated answer and count the occurrences of each token\n",
        "    predicted_tokens = Counter(get_tokens(generated_answer))\n",
        "    num_predicted = sum(predicted_tokens.values())  # Total number of tokens in the generated answer\n",
        "\n",
        "    max_recall, max_f1 = 0, 0  # Initialize maximum recall and F1 score\n",
        "    for gold_answer in gold_answer_list:\n",
        "        gold_tokens = Counter(get_tokens(gold_answer))  # Tokenize and count tokens in the gold answer\n",
        "        num_gold = sum(gold_tokens.values())  # Total number of tokens in the gold answer\n",
        "        num_same = sum((predicted_tokens & gold_tokens).values())  # Count overlapping tokens\n",
        "\n",
        "        if num_same == 0:  # Skip if there are no overlapping tokens\n",
        "            continue\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        precision = 1.0 * num_same / num_predicted\n",
        "        recall = 1.0 * num_same / num_gold\n",
        "\n",
        "        # Update maximum recall and F1 score\n",
        "        max_recall = max(recall, max_recall)\n",
        "        max_f1 = max(((2 * precision * recall) / (precision + recall)), max_f1)\n",
        "\n",
        "    return max_recall, max_f1\n",
        "\n",
        "def compute_recall_f1(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Compute average recall and F1 score using parallel processing.\n",
        "    - Processes each pair of gold and generated answers in parallel.\n",
        "    - Returns the average recall and F1 score as percentages.\n",
        "    \"\"\"\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        # Use multiprocessing to compute recall and F1 scores for all pairs\n",
        "        results = pool.map(compute_recall_f1_single, zip(gold_answers, generated_answers))\n",
        "\n",
        "    # Sum up recall and F1 scores\n",
        "    total_recall, total_f1 = map(sum, zip(*results))\n",
        "    avg_recall = 100 * total_recall / len(gold_answers)  # Calculate average recall\n",
        "    avg_f1 = 100 * total_f1 / len(gold_answers)  # Calculate average F1 score\n",
        "\n",
        "    return avg_recall, avg_f1\n",
        "\n",
        "def evaluate(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Evaluate generated answers against gold answers.\n",
        "    - Computes exact match, recall, and F1 score.\n",
        "    - Returns a dictionary with the evaluation metrics.\n",
        "    \"\"\"\n",
        "    exact_match = compute_exact_match(gold_answers, generated_answers)\n",
        "    answer_recall, f1_score_avg = compute_recall_f1(gold_answers, generated_answers)\n",
        "\n",
        "    return {\n",
        "        \"Exact Match\": exact_match,\n",
        "        \"F1 Score\": f1_score_avg,\n",
        "        \"Answer Recall\": answer_recall\n",
        "    }\n",
        "\n",
        "def run_evaluate(combined_dir=None, gold_answer_dir=None, generated_answer_dir=None, output_dir=None):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the generated answers.\n",
        "\n",
        "    Args:\n",
        "        combined_dir (str): Path to the CSV file containing combined gold and generated answers.\n",
        "        gold_answer_dir (str): Path to the file containing the gold answers.\n",
        "        generated_answer_dir (str): Path to the file containing the generated answers.\n",
        "        output_dir (str): Path to save the evaluation results.\n",
        "    \"\"\"\n",
        "    if not output_dir:\n",
        "        raise ValueError(\"The 'output_dir' argument is required.\")\n",
        "\n",
        "    if combined_dir:\n",
        "        # Read combined data from a CSV file\n",
        "        generation_df = pd.read_csv(combined_dir)\n",
        "        generated_answers = generation_df[\"Generated_Answer\"].tolist()  # Extract generated answers\n",
        "        gold_answers = generation_df[\"Reference_Answers\"].apply(lambda x: str(x).split(\"[SEP]\")).tolist()  # Extract gold answers\n",
        "        print(gold_answers[:5])  # Print the first 5 gold answers for verification\n",
        "        print(\"Loaded combined gold and generated answers from CSV files.\")\n",
        "    else:\n",
        "        # Read gold and generated answers from separate files\n",
        "        if not gold_answer_dir or not generated_answer_dir:\n",
        "            raise ValueError(\"Both 'gold_answer_dir' and 'generated_answer_dir' arguments are required if 'combined_dir' is not provided.\")\n",
        "\n",
        "        # Read gold answers from a file\n",
        "        with open(gold_answer_dir, \"r\") as f:\n",
        "            gold_answers = [line.strip().split(\";\") for line in f]\n",
        "\n",
        "        # Read generated answers from a file\n",
        "        with open(generated_answer_dir, \"r\") as f:\n",
        "            generated_answers = [line.strip() for line in f]\n",
        "\n",
        "    # Evaluate the generated answers\n",
        "    results = evaluate(gold_answers, generated_answers)\n",
        "    print(f\"Evaluation results: {results}\")\n",
        "\n",
        "    # Save the evaluation results to a JSON file\n",
        "    with open(output_dir, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "# Run the evaluation with specified input and output paths\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_baseline.csv\", output_dir=\"./results/tests/llama3_baseline.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7KO4MIIq6kW"
      },
      "outputs": [],
      "source": [
        "# Baseline performance\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_baseline.csv\", output_dir=\"./results/tests/llama3_baseline.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chroma_top3.csv\", output_dir=\"./results/tests/llama3_recursive_chroma_top3.json\")\n",
        "\n",
        "# For hyperparameter tuning on chunk size\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_recursive_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chunk2000_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_recursive_chunk2000_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chunk500_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_recursive_chunk500_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chunk750_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_recursive_chunk750_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_recursive_chunk1500_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_recursive_chunk1500_chroma_top3_sample100.json\")\n",
        "\n",
        "# For hyperparameter tuning on splitter\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_character_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_character_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_tokensplit_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_tokensplit_chroma_top3_sample100.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_semantic_chroma_top3_sample100.csv\", output_dir=\"./results/tests/llama3_semantic_chroma_top3_sample100.json\")\n",
        "\n",
        "# For tuning reranking using faiss\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test.csv\", output_dir=\"./results/tests/llama3_faiss_test.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_rerank.csv\", output_dir=\"./results/tests/llama3_faiss_test_rerank.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_rerank_t5.csv\", output_dir=\"./results/tests/llama3_faiss_test_rerank_t5.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_rerank_MiniLM.csv\", output_dir=\"./results/tests/llama3_faiss_test_rerank_MiniLM.json\")\n",
        "\n",
        "# For tuning hypo_doc retrieval\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_hypo.csv\", output_dir=\"./results/tests/llama3_faiss_test_hypo.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_hypo_promptENG.csv\", output_dir=\"./results/tests/llama3_faiss_test_hypo_promptENG.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_hypo_promptENG2.csv\", output_dir=\"./results/tests/llama3_faiss_test_hypo_promptENG2.json\")\n",
        "run_evaluate(combined_dir=\"./output/tests/llama3_faiss_test_hypo_promptENG3.csv\", output_dir=\"./results/tests/llama3_faiss_test_hypo_promptENG3.json\")\n",
        "\n",
        "# For running evaluation on the full 3900 test set\n",
        "run_evaluate(combined_dir=\"./output/qa3000/llama3_faiss_rerank.csv\", output_dir=\"./results/qa3000/llama3_faiss_rerank.json\")\n",
        "run_evaluate(combined_dir=\"./output/qa3000/llama3_faiss_rerank_sublink.csv\", output_dir=\"./results/qa3000/llama3_faiss_rerank_sublink.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
