{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJO-2CLihs3"
      },
      "source": [
        "# Significance Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ActBx_a9OpKQ",
        "outputId": "336e538d-e064-4484-d938-94c67bb0e905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Kansas'], ['3 years'], ['1897'], ['Several'], ['Appalachia']]\n",
            "{'Exact Match': 9.395632300660234, 'F1 Score': 17.287144413355495, 'Answer Recall': 18.152428274726216}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# Constants\n",
        "WHITESPACE_AND_PUNCTUATION = set(string.whitespace + string.punctuation)\n",
        "ARTICLES = set(['the', 'a', 'an'])\n",
        "\n",
        "def clean_answer(answer):\n",
        "    \"\"\"Clean and normalize an answer.\"\"\"\n",
        "    answer = str(answer).lower()\n",
        "    answer = answer.replace(u'\\u00a0', ' ')  # Replace non-breaking space\n",
        "    answer = answer.strip(string.whitespace + string.punctuation)  # Strip whitespace and punctuation\n",
        "    answer = ' '.join([word for word in answer.split() if word not in ARTICLES])  # Remove articles\n",
        "    return answer\n",
        "\n",
        "def compute_exact_match_single(gold_answer_list, generated_answer):\n",
        "    \"\"\"Check if the generated answer exactly matches any of the gold answers.\"\"\"\n",
        "    cleaned_generated = clean_answer(generated_answer)\n",
        "    return any(clean_answer(gold) == cleaned_generated for gold in gold_answer_list)\n",
        "\n",
        "def compute_exact_match(gold_answers, generated_answers):\n",
        "    \"\"\"Compute exact match score using vectorized operations.\"\"\"\n",
        "    exact_match = sum(compute_exact_match_single(gold, gen) for gold, gen in zip(gold_answers, generated_answers))\n",
        "    return 100 * exact_match / len(gold_answers)\n",
        "\n",
        "def compute_recall_f1_single(args):\n",
        "    \"\"\"Compute recall and F1 score for a single pair of gold and generated answers.\"\"\"\n",
        "    gold_answer_list, generated_answer = args\n",
        "\n",
        "    def get_tokens(text):\n",
        "        text = clean_answer(text)\n",
        "        for delimiter in WHITESPACE_AND_PUNCTUATION:\n",
        "            text = text.replace(delimiter, ' ')\n",
        "        return text.split()\n",
        "\n",
        "    predicted_tokens = Counter(get_tokens(generated_answer))\n",
        "    num_predicted = sum(predicted_tokens.values())\n",
        "\n",
        "    max_recall, max_f1 = 0, 0\n",
        "    for gold_answer in gold_answer_list:\n",
        "        gold_tokens = Counter(get_tokens(gold_answer))\n",
        "        num_gold = sum(gold_tokens.values())\n",
        "        num_same = sum((predicted_tokens & gold_tokens).values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            continue\n",
        "        precision = num_same / num_predicted\n",
        "        recall = num_same / num_gold\n",
        "        max_recall = max(recall, max_recall)\n",
        "        max_f1 = max(2 * precision * recall / (precision + recall), max_f1)\n",
        "\n",
        "    return max_recall, max_f1\n",
        "\n",
        "def compute_recall_f1(gold_answers, generated_answers):\n",
        "    \"\"\"Compute average recall and F1 score using parallel processing.\"\"\"\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        results = pool.map(compute_recall_f1_single, zip(gold_answers, generated_answers))\n",
        "\n",
        "    total_recall, total_f1 = map(sum, zip(*results))\n",
        "    avg_recall = 100 * total_recall / len(gold_answers)\n",
        "    avg_f1 = 100 * total_f1 / len(gold_answers)\n",
        "    return avg_recall, avg_f1\n",
        "\n",
        "def evaluate(gold_answers, generated_answers):\n",
        "    \"\"\"Evaluate generated answers against gold answers.\"\"\"\n",
        "    exact_match = compute_exact_match(gold_answers, generated_answers)\n",
        "    answer_recall, f1_score_avg = compute_recall_f1(gold_answers, generated_answers)\n",
        "\n",
        "    return {\n",
        "        \"Exact Match\": exact_match,\n",
        "        \"F1 Score\": f1_score_avg,\n",
        "        \"Answer Recall\": answer_recall\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        # Specify the paths directly\n",
        "    combined_dir = \"./output/tests/llama3_baseline.csv\"  # Path to the combined gold and generated answers CSV file\n",
        "    gold_answer_dir = \"\"  # Path to the gold answers text file\n",
        "    generated_answer_dir = \"\"  # Path to the generated answers text file\n",
        "    output_dir = \"./results/tests/llama3_baseline.json\"  # Path to save the evaluation results\n",
        "\n",
        "    if combined_dir:\n",
        "        # read in as csv file\n",
        "        generation_df = pd.read_csv(combined_dir)\n",
        "        generated_answers = generation_df[\"Generated_Answer\"].tolist()\n",
        "        # each row is a list of gold answers\n",
        "        # example gold answers: [\"William Pitt\", \"William Pitt the Younger\"]\n",
        "        gold_answers = generation_df[\"Reference_Answers\"].apply(lambda x: str(x).split(\"[SEP]\")).tolist()\n",
        "        print(gold_answers[:5])\n",
        "\n",
        "    else:\n",
        "        # read the gold answers, create a list of lists\n",
        "        # each sublist contains one or more gold answers\n",
        "        gold_answers = []\n",
        "        with open(gold_answer_dir, 'r') as f:\n",
        "            for line in f:\n",
        "                gold_answers.append(line.strip().split(';'))\n",
        "\n",
        "        # read the generated answers, each line contains one generated answer\n",
        "        generated_answers = []\n",
        "        with open(generated_answer_dir, 'r') as f:\n",
        "            for line in f:\n",
        "                generated_answers.append(line.strip())\n",
        "\n",
        "    # Evaluate\n",
        "    results = evaluate(gold_answers, generated_answers)\n",
        "    print(results)\n",
        "\n",
        "    # Save results\n",
        "    with open(output_dir, 'w') as f:\n",
        "        json.dump(results, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
